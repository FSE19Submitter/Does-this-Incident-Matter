{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\v-jux\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import autograd, gluon, init, metric, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import re\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['id', 'component', 'title', 'desp', 'product', 'severity', '6', '7', '8', '9','10']\n",
    "PATH = './data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'sample_sev0.txt', header=0, index_col=None, names=COLUMNS)\n",
    "test = pd.read_csv(PATH + 'sample_sev10.txt', header=0, index_col=None, names=COLUMNS)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    train = pd.concat([train, pd.read_csv(PATH + 'sample_sev' + str(i) + '.txt', header=0, index_col=None, names=COLUMNS)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "major       631\n",
      "minor       622\n",
      "critical    455\n",
      "trivial     400\n",
      "blocker     242\n",
      "Name: severity, dtype: int64\n",
      "major       63\n",
      "minor       62\n",
      "critical    48\n",
      "trivial     40\n",
      "blocker     22\n",
      "Name: severity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['severity'].value_counts())\n",
    "print(test['severity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trivial', 'minor', 'major', 'critical', 'blocker'])\n"
     ]
    }
   ],
   "source": [
    "LABELS = {\n",
    "    \"trivial\": 0,\n",
    "    \"minor\": 1,\n",
    "    \"major\": 2,\n",
    "    \"critical\": 3,\n",
    "    \"blocker\": 4\n",
    "}\n",
    "print(LABELS.keys())\n",
    "\n",
    "train = train[ train['severity'].isin(LABELS.keys()) ].replace(LABELS)\n",
    "test = test[ test['severity'].isin(LABELS.keys()) ].replace(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r'\\\\n',' ',string)\n",
    "    string = re.sub(r'\\\\r',' ',string)\n",
    "    string = re.sub(r'\\\\t',' ',string)\n",
    "    string = re.sub(r'</?\\w+[^>]*>',' ',string)\n",
    "    string = re.sub(r'[^A-Za-z0-9(),!?\\'\\`]', ' ', string)    \n",
    "    string = re.sub(r'\\'s', ' \\'s', string)\n",
    "    string = re.sub(r'\\'ve', ' \\'ve', string)\n",
    "    string = re.sub(r'n\\'t', ' n\\'t', string)\n",
    "    string = re.sub(r'\\'re', ' \\'re', string)\n",
    "    string = re.sub(r'\\'d', ' \\'d', string)\n",
    "    string = re.sub(r'\\'ll', ' \\'ll', string)\n",
    "    string = re.sub(r',', ' , ', string)\n",
    "    string = re.sub(r'!', ' ! ', string)\n",
    "    string = re.sub(r'\\(', ' \\( ', string)\n",
    "    string = re.sub(r'\\)', ' \\) ', string)\n",
    "    string = re.sub(r'\\?', ' \\? ', string)\n",
    "    string = re.sub(r'\\s+', ' ', string)   \n",
    "    return string.strip().lower().split(' ')\n",
    "\n",
    "train['text'] = train.title.map(str) + \" \" + train.desp\n",
    "test['text'] = test.title.map(str) + \" \" + test.desp\n",
    "\n",
    "train['text'] = train['text'].map(lambda x: clean_str(str(x)))\n",
    "test['text'] = test['text'].map(lambda x: clean_str(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(preds, truths, n, model):\n",
    "    best_n = np.argsort(preds, axis=1)[:,-n:]\n",
    "    ts = truths\n",
    "    successes = 0\n",
    "    for i in range(len(ts)):\n",
    "        if ts[i] in [model.classes_[line] for line in best_n[i,:]]:\n",
    "            successes += 1\n",
    "    return float(successes)/len(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(train_tokenized, token_counter):\n",
    "    for sample in train_tokenized:\n",
    "        for token in sample:\n",
    "            if token not in token_counter:\n",
    "                token_counter[token] = 1\n",
    "            else:\n",
    "                token_counter[token] += 1\n",
    "    return token_counter\n",
    "\n",
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in vocab.token_to_idx:\n",
    "                feature.append(vocab.token_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)         \n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=1000, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) > maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            # 添加 PAD 符号使每个序列等长（长度为 maxlen ）。\n",
    "            while len(padded_feature) < maxlen:\n",
    "                padded_feature.append(PAD)\n",
    "        padded_feature  = [0,0,0,0] + padded_feature + [0,0,0,0]\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_do(project_name, dropout_rate, train_df, test_df): \n",
    "    print('====')\n",
    "    print(project_name)\n",
    "    print('====')\n",
    "    \n",
    "    feature_columns = ['component', 'product']\n",
    "    \n",
    "    csv_write = csv.writer(open('DeepTIP-Mozilla.csv','a',newline=''), dialect='excel')\n",
    "    def count_token(train_tokenized, token_counter):\n",
    "        for sample in train_tokenized:\n",
    "            for token in sample:\n",
    "                if token not in token_counter:\n",
    "                    token_counter[token] = 1\n",
    "                else:\n",
    "                    token_counter[token] += 1\n",
    "        return token_counter\n",
    "    \n",
    "    def encode_samples(tokenized_samples, vocab):\n",
    "        features = []\n",
    "        for sample in tokenized_samples:\n",
    "            feature = []\n",
    "            for token in sample:\n",
    "                if token in vocab.token_to_idx:\n",
    "                    feature.append(vocab.token_to_idx[token])\n",
    "                else:\n",
    "                    feature.append(0)\n",
    "            features.append(feature)         \n",
    "        return features\n",
    "\n",
    "    def pad_samples(features, maxlen=1000, PAD=0):\n",
    "        padded_features = []\n",
    "        for feature in features:\n",
    "            if len(feature) > maxlen:\n",
    "                padded_feature = feature[:maxlen]\n",
    "            else:\n",
    "                padded_feature = feature\n",
    "                # 添加 PAD 符号使每个序列等长 长度为 maxlen\n",
    "                while len(padded_feature) < maxlen:\n",
    "                    padded_feature.append(PAD)\n",
    "            padded_feature  = [0,0,0,0] + padded_feature + [0,0,0,0]\n",
    "            padded_features.append(padded_feature)\n",
    "        return padded_features\n",
    "    \n",
    "    class TextCNN(nn.Block):\n",
    "        def __init__(self, vocab, embedding_size, ngram_kernel_sizes, nums_channels, num_outputs, sequence_len,max_len_dict, **kwargs):\n",
    "            \n",
    "            super(TextCNN, self).__init__(**kwargs)\n",
    "            self.ngram_kernel_sizes = ngram_kernel_sizes\n",
    "            self.nums_channels = nums_channels\n",
    "            self.embedding = nn.Embedding(len(vocab), embedding_size)\n",
    "            self.embedding_component = nn.Embedding(max_len_dict['component'], 10)\n",
    "            self.embedding_product = nn.Embedding(max_len_dict['product'], 48)\n",
    "            \n",
    "            for i in range(len(ngram_kernel_sizes)):\n",
    "                conv = nn.Conv1D(nums_channels[i],\n",
    "                    kernel_size=ngram_kernel_sizes[i],\n",
    "                    strides=1,\n",
    "                    activation='relu')\n",
    "                pool = nn.GlobalMaxPool1D()\n",
    "                setattr(self, f'conv_{i}', conv) \n",
    "                setattr(self, f'pool_{i}', pool)\n",
    "\n",
    "            self.temp_pool = nn.GlobalMaxPool1D()\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.dropout2 = nn.Dropout(dropout_rate)\n",
    "            self.dense = nn.Dense(32)  \n",
    "            self.decoder = nn.Dense(num_outputs)\n",
    "       \n",
    "        def forward(self, inputs, component, product):\n",
    "\n",
    "            embeddings = self.embedding(inputs)\n",
    "            embedd_component = self.embedding_component(component)\n",
    "            embedd_product = self.embedding_product(product)\n",
    "            \n",
    "            # TARGET\n",
    "            embeddings = embeddings.transpose((0,2,1)) \n",
    "            encoding = [\n",
    "                nd.flatten(self.get_pool(i)(self.get_conv(i)(embeddings)))\n",
    "                for i in range(len(self.ngram_kernel_sizes))]\n",
    "            encoding = nd.concat(*encoding, dim=1)\n",
    "\n",
    "            outputs = self.dense(self.dropout(\n",
    "                nd.concat(\n",
    "                encoding,\n",
    "                embedd_component,\n",
    "                embedd_product,\n",
    "                dim=1)               \n",
    "            ))\n",
    "            \n",
    "            outs = self.decoder(self.dropout2(outputs))\n",
    "            \n",
    "            return outs\n",
    "        \n",
    "        def get_conv(self, i):\n",
    "            return getattr(self, f'conv_{i}')\n",
    "        \n",
    "        def get_pool(self, i):\n",
    "            return getattr(self, f'pool_{i}')\n",
    "        \n",
    "        def get_conv_c(self, i):\n",
    "            return getattr(self, f'conv_c_{i}')\n",
    "        \n",
    "        def get_pool_c(self, i):\n",
    "            return getattr(self, f'pool_c_{i}')\n",
    "        \n",
    "        def attention_model(self, attention_size):\n",
    "            model = nn.Sequential()\n",
    "            model.add(nn.Dense(attention_size, activation='tanh', use_bias=False, flatten=False),\n",
    "                      nn.Dense(1, use_bias=False, flatten=False))\n",
    "            return model\n",
    "    \n",
    "    def eval_model(train_iter, show = False):\n",
    "        l_sum = 0\n",
    "        l_n = 0\n",
    "        \n",
    "        pred_probs = []\n",
    "        labels_result = []\n",
    "        pred_result = []\n",
    "        \n",
    "        for data, label, weights, component, product in train_iter:\n",
    "            X = data.as_in_context(ctx)\n",
    "            y = label.as_in_context(ctx).T \n",
    "            \n",
    "            component = component.as_in_context(ctx)\n",
    "            product = product.as_in_context(ctx)\n",
    "            \n",
    "            y_pred = net(X, component, product)\n",
    "            l = loss(y_pred, y)\n",
    "            \n",
    "            pred_result.extend(list(nd.argmax(nd.softmax(y_pred), axis=1).astype('int').asnumpy()))\n",
    "            labels_result.extend(list(y.astype('int').asnumpy()))\n",
    "   \n",
    "        if show == True:\n",
    "            print(metrics.classification_report(labels_result, pred_result, target_names=LABELS.keys()))\n",
    "        \n",
    "        return 1, metrics.f1_score(labels_result, pred_result, average='macro')\n",
    "    \n",
    "    def eval_model_test(train_iter):\n",
    "        l_sum = 0\n",
    "        l_n = 0\n",
    "        \n",
    "        pred_result = []\n",
    "        labels_result = []\n",
    "        pred_probs = []\n",
    "        \n",
    "        for data, label, weights, component, product in train_iter:\n",
    "            X = data.as_in_context(ctx)\n",
    "            y = label.as_in_context(ctx).T    \n",
    "            \n",
    "            component = component.as_in_context(ctx)\n",
    "            product = product.as_in_context(ctx)\n",
    "            \n",
    "            y_pred= net(X, component, product)           \n",
    "            l = loss(y_pred, y)\n",
    "            \n",
    "            pred_result.extend(list(nd.argmax(nd.softmax(y_pred), axis=1).astype('int').asnumpy()))\n",
    "            labels_result.extend(list(y.astype('int').asnumpy()))\n",
    "        \n",
    "        print(metrics.classification_report(labels_result, pred_result, target_names=LABELS.keys()))\n",
    "        \n",
    "        return metrics.precision_score(labels_result, pred_result, average='macro'), \\\n",
    "                    metrics.recall_score(labels_result, pred_result, average='macro'),\\\n",
    "                    metrics.f1_score(labels_result, pred_result, average='macro'),\\\n",
    "                    metrics.accuracy_score(labels_result, pred_result), \\\n",
    "                    pred_result,pred_probs,labels_result\n",
    "\n",
    "    samples = train_df\n",
    "    samples_test = test_df\n",
    "    \n",
    "    samples = pd.concat([samples, samples[samples['severity'] == 0]])\n",
    "    samples = pd.concat([samples, samples[samples['severity'] == 0]])\n",
    "    samples = pd.concat([samples, samples[samples['severity'] == 4]])\n",
    "    \n",
    "    train = {}\n",
    "    test = {}\n",
    "    \n",
    "    train['sentences'] = list(samples['text'])\n",
    "    train['labels'] = list(samples['severity'])\n",
    "    \n",
    "    # multi weight for train\n",
    "    weight_dict = {}\n",
    "    total = len(samples['severity'])\n",
    "    min_sample = samples['severity'].value_counts().min()\n",
    "    for index,x in samples['severity'].value_counts().iteritems():\n",
    "        weight_dict[index] = 1 - float(x) / total\n",
    "    \n",
    "    # Softmax\n",
    "    #weightMatrix = nd.array([weight_dict[x] for x in weight_dict])\n",
    "    #weightMatrix = nd.softmax(weightMatrix)\n",
    "    \n",
    "    #i=0\n",
    "    #for x in weight_dict:\n",
    "    #    weight_dict[x] = weightMatrix[i].astype('float').asnumpy()[0]\n",
    "    #    i += 1\n",
    "    \n",
    "    train['weights'] = []\n",
    "    for line in samples['severity']:\n",
    "        train['weights'].append(weight_dict[line])\n",
    "    train['weights'] =  nd.array(train['weights'])\n",
    "                        \n",
    "    test['sentences'] = list(samples_test['text']) \n",
    "    test['labels'] = list(samples_test['severity'])\n",
    "    \n",
    "    test['weights'] = []\n",
    "    for line in samples_test['severity']:  \n",
    "         test['weights'].append(weight_dict[line])\n",
    "    test['weights'] = nd.array(test['weights'])\n",
    "    \n",
    "    max_len = 80\n",
    "    while (np.sum(np.array(list(map(len,train['sentences'])))<max_len)/len(train['sentences'])<0.95):\n",
    "        max_len += 10   \n",
    "    \n",
    "\n",
    "    for c in feature_columns:\n",
    "        tmp_dict = {}\n",
    "        idx = 0\n",
    "        for x in list(samples[c].drop_duplicates()):\n",
    "            tmp_dict[x] = idx\n",
    "            idx += 1\n",
    "            \n",
    "        for x in list(samples_test[c].drop_duplicates()):\n",
    "            if x not in tmp_dict:\n",
    "                tmp_dict[x] = idx\n",
    "                idx += 1\n",
    "            \n",
    "        train[c] = nd.array(list(samples[c].replace(tmp_dict)))\n",
    "        test[c] = nd.array(list(samples_test[c].replace(tmp_dict)))\n",
    "\n",
    "    token_counter = collections.Counter()\n",
    "    token_counter = count_token(train['sentences'], token_counter)\n",
    "    vocab = text.vocab.Vocabulary(token_counter, unknown_token='<unk>', min_freq=2, reserved_tokens=None, most_freq_count=30000)\n",
    "    \n",
    "    ctx = mx.gpu(1)\n",
    "    from gensim.models import FastText\n",
    "    model = FastText(train['sentences'] + test['sentences'], min_count=1, size=200, iter=10, workers=8)\n",
    "    w2v = []\n",
    "    for line in vocab.token_to_idx:\n",
    "        if line in model.wv:\n",
    "            w2v.append(model.wv[line])\n",
    "        else:\n",
    "            w2v.append(np.zeros(200))\n",
    "    w2v = nd.array(w2v)\n",
    "    print('w2v success', w2v.shape)\n",
    "    \n",
    "    train['features'] = encode_samples(train['sentences'], vocab)\n",
    "    train['features'] = nd.array(pad_samples(train['features'], max_len, 0))\n",
    "                                \n",
    "    test['features'] = encode_samples(test['sentences'], vocab)\n",
    "    test['features'] = nd.array(pad_samples(test['features'], max_len, 0))\n",
    "    \n",
    "    max_len_dict = {}\n",
    "    for c in feature_columns:\n",
    "        max_len = int(np.max(train[c]).asnumpy()) \n",
    "        if max_len <int(np.max(test[c]).asnumpy()) :\n",
    "            max_len = int(np.max(test[c]).asnumpy())\n",
    "        max_len_dict[c] = max_len\n",
    "    print('encode_samples success')\n",
    "\n",
    "    sequence_length = 0\n",
    "    lr = 0.001\n",
    "    num_epochs = 30\n",
    "    sentence_hidden = 32\n",
    "    batch_size = 512\n",
    "    embed_size = 200\n",
    "    ngram_kernel_sizes = [3,4,5]\n",
    "    nums_channels = [20,20,20]\n",
    "    \n",
    "    net = TextCNN(vocab, embed_size, ngram_kernel_sizes, nums_channels, 5, sequence_length, max_len_dict)\n",
    "    net.initialize(init.Xavier(), ctx=ctx)\n",
    "    net.embedding.weight.set_data(w2v.as_in_context(ctx))\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        train['features'],\n",
    "        train['labels'],\n",
    "        train['weights'],\n",
    "        train['component'],\n",
    "        train['product']\n",
    "    ), batch_size, shuffle=True)\n",
    "    \n",
    "    test_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        test['features'],\n",
    "        test['labels'],\n",
    "        test['weights'],\n",
    "        test['component'],\n",
    "        test['product']\n",
    "    ), batch_size, shuffle=True)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    start = time.clock()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        l_sum = 0\n",
    "        l_n = 0\n",
    "        for data, label, weight, component, product in train_iter:\n",
    "            X = data.as_in_context(ctx)\n",
    "            y = label.as_in_context(ctx).T\n",
    "            w = weight.as_in_context(ctx).T\n",
    "            \n",
    "            component = component.as_in_context(ctx)\n",
    "            product = product.as_in_context(ctx)\n",
    "            \n",
    "            with autograd.record():\n",
    "                y_hat = net(X, component, product)\n",
    "                l = loss(y_hat, y, w)\n",
    "            \n",
    "            l.backward()\n",
    "            trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        \n",
    "        train_loss, train_f1 = eval_model(train_iter)\n",
    "        val_loss, val_f1 = eval_model(test_iter)#, show=True\n",
    "        if val_f1>=best_f1:\n",
    "            net.save_parameters('DeepTIP.model')\n",
    "            best_f1 = val_f1\n",
    "        print('epoch %d, train f1 %.2f; val f1 %.4f' \n",
    "              % (epoch, train_f1, val_f1))\n",
    "    \n",
    "    end = time.clock()\n",
    "    print('train success')\n",
    "   \n",
    "    net.load_parameters('DeepTIP.model')\n",
    "    start_p = time.clock()\n",
    "    pre, recall, f1, acc1, pred_result, pred_probs, labels_result = eval_model_test(test_iter)\n",
    "    end_p = time.clock()\n",
    "    \n",
    "    csv_write.writerow([project_name,'DeepTip', pre, recall, f1, acc1,end-start,end_p-start_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "Mozilla\n",
      "====\n",
      "w2v success (1917, 200)\n",
      "encode_samples success\n",
      "epoch 1, train f1 0.35; val f1 0.3475\n",
      "epoch 2, train f1 0.42; val f1 0.4779\n",
      "epoch 3, train f1 0.47; val f1 0.5011\n",
      "epoch 4, train f1 0.50; val f1 0.4782\n",
      "epoch 5, train f1 0.52; val f1 0.4847\n",
      "epoch 6, train f1 0.54; val f1 0.4943\n",
      "epoch 7, train f1 0.56; val f1 0.5170\n",
      "epoch 8, train f1 0.59; val f1 0.4973\n",
      "epoch 9, train f1 0.60; val f1 0.4854\n",
      "epoch 10, train f1 0.63; val f1 0.5182\n",
      "epoch 11, train f1 0.62; val f1 0.4735\n",
      "epoch 12, train f1 0.68; val f1 0.5011\n",
      "epoch 13, train f1 0.67; val f1 0.5143\n",
      "epoch 14, train f1 0.70; val f1 0.5007\n",
      "epoch 15, train f1 0.72; val f1 0.5018\n",
      "epoch 16, train f1 0.74; val f1 0.5046\n",
      "epoch 17, train f1 0.75; val f1 0.5146\n",
      "epoch 18, train f1 0.77; val f1 0.5179\n",
      "epoch 19, train f1 0.77; val f1 0.5398\n",
      "epoch 20, train f1 0.79; val f1 0.5451\n",
      "train success\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    trivial       0.22      0.10      0.14        40\n",
      "      minor       0.48      0.92      0.63        62\n",
      "      major       0.60      0.54      0.57        63\n",
      "   critical       0.85      0.48      0.61        48\n",
      "    blocker       1.00      0.64      0.78        22\n",
      "\n",
      "avg / total       0.59      0.56      0.54       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_do('Mozilla', 0.4, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
