{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import autograd, gluon, init, metric, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import re\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(preds, truths, n, model):\n",
    "    best_n = np.argsort(preds, axis=1)[:,-n:]\n",
    "    ts = truths\n",
    "    successes = 0\n",
    "    for i in range(len(ts)):\n",
    "        if ts[i] in [model.classes_[line] for line in best_n[i,:]]:\n",
    "            successes += 1\n",
    "    return float(successes)/len(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(train_tokenized, token_counter):\n",
    "    for sample in train_tokenized:\n",
    "        for token in sample:\n",
    "            if token not in token_counter:\n",
    "                token_counter[token] = 1\n",
    "            else:\n",
    "                token_counter[token] += 1\n",
    "    return token_counter\n",
    "\n",
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in vocab.token_to_idx:\n",
    "                feature.append(vocab.token_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)         \n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=1000, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) > maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            # 添加 PAD 符号使每个序列等长（长度为 maxlen ）。\n",
    "            while len(padded_feature) < maxlen:\n",
    "                padded_feature.append(PAD)\n",
    "        padded_feature  = [0,0,0,0] + padded_feature + [0,0,0,0]\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNABLE TO REPRODUCE': 0, 'BY DESIGN': 1, \"WON'T FIX\": 2, 'FALSE ALARM': 3, 'TRANSIENT': 4, 'CUSTOMER ERROR': 5, 'NEEDFIXED': 6}\n"
     ]
    }
   ],
   "source": [
    "TRUE_LABEL = [\n",
    "    'Transient', \n",
    "    'False Alarm', \n",
    "    'Won\\'t Fix', \n",
    "    'Unable To Reproduce', \n",
    "    'Customer Error',\n",
    "    'Won\\'t fix',\n",
    "    'By Design',\n",
    "]\n",
    "TRUE_LABEL = list(set([i.upper() for i in TRUE_LABEL]))\n",
    "\n",
    "LABELDICT = {}\n",
    "for i in range(len(TRUE_LABEL)):\n",
    "    LABELDICT[TRUE_LABEL[i]] = i\n",
    "LABELDICT['NeedFixed'.upper()] = len(TRUE_LABEL)\n",
    "print(LABELDICT)    \n",
    "\n",
    "def getBinaryLabel(x):\n",
    "    if x.upper() in TRUE_LABEL:\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def getMultiLabel(x):\n",
    "    return LABELDICT[x.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'MonitorId',\n",
    "    'OccurringEnvironment',\n",
    "    'TenantSilos',\n",
    "    'OccurringDatacenter',\n",
    "    'OccurringDeviceName',\n",
    "    'OccurringDeviceGroup',\n",
    "    'OccurringServiceInstanceId',\n",
    "    'RaisingEnvironment',\n",
    "    'RaisingDatacenter',\n",
    "    'RaisingDeviceName',\n",
    "    'RaisingDeviceGroup',\n",
    "    'IncidentType',\n",
    "    'SourceType',\n",
    "    'SourceOrigin',\n",
    "    'variable'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_dict.pkl','rb') as file:\n",
    "    twitter_dict=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(w):\n",
    "    if ' ' not in w: \n",
    "        if w not in twitter_dict:\n",
    "            return w\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return (' ').join([x for x in w.split(' ') if x not in twitter_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_do(project_name, dropout_rate): \n",
    "    print('====')\n",
    "    print(project_name)\n",
    "    print('====')\n",
    "    \n",
    "    csv_write = csv.writer(open('DeepTIP-Attention.csv','a',newline=''), dialect='excel')\n",
    "    def count_token(train_tokenized, token_counter):\n",
    "        for sample in train_tokenized:\n",
    "            for token in sample:\n",
    "                if token not in token_counter:\n",
    "                    token_counter[token] = 1\n",
    "                else:\n",
    "                    token_counter[token] += 1\n",
    "        return token_counter\n",
    "    \n",
    "    def encode_samples(tokenized_samples, vocab):\n",
    "        features = []\n",
    "        for sample in tokenized_samples:\n",
    "            feature = []\n",
    "            for token in sample:\n",
    "                if token in vocab.token_to_idx:\n",
    "                    feature.append(vocab.token_to_idx[token])\n",
    "                else:\n",
    "                    feature.append(0)\n",
    "            features.append(feature)         \n",
    "        return features\n",
    "\n",
    "    def pad_samples(features, maxlen=1000, PAD=0):\n",
    "        padded_features = []\n",
    "        for feature in features:\n",
    "            if len(feature) > maxlen:\n",
    "                padded_feature = feature[:maxlen]\n",
    "            else:\n",
    "                padded_feature = feature\n",
    "                # 添加 PAD 符号使每个序列等长（长度为 maxlen ）。\n",
    "                while len(padded_feature) < maxlen:\n",
    "                    padded_feature.append(PAD)\n",
    "            padded_feature  = [0,0,0,0] + padded_feature + [0,0,0,0]\n",
    "            padded_features.append(padded_feature)\n",
    "        return padded_features\n",
    "    \n",
    "    class TextCNN(nn.Block):\n",
    "        def __init__(self, vocab, embedding_size, ngram_kernel_sizes, nums_channels, num_outputs, max_len_dict, sequence_len, **kwargs):\n",
    "            \n",
    "            super(TextCNN, self).__init__(**kwargs)\n",
    "            self.ngram_kernel_sizes = ngram_kernel_sizes\n",
    "            self.nums_channels = nums_channels\n",
    "            self.embedding = nn.Embedding(len(vocab), embedding_size)            \n",
    "            self.embedding_MonitorId = nn.Embedding(max_len_dict['MonitorId'], 16)\n",
    "            self.embedding_OccurringEnvironment = nn.Embedding(max_len_dict['OccurringEnvironment'], 16)\n",
    "            self.embedding_TenantSilos = nn.Embedding(max_len_dict['TenantSilos'], 16)\n",
    "            self.embedding_OccurringDatacenter = nn.Embedding(max_len_dict['OccurringDatacenter'], 4)\n",
    "            self.embedding_OccurringDeviceName = nn.Embedding(max_len_dict['OccurringDeviceName'], 16)\n",
    "            self.embedding_OccurringDeviceGroup = nn.Embedding(max_len_dict['OccurringDeviceGroup'], 4)\n",
    "            self.embedding_OccurringServiceInstanceId = nn.Embedding(max_len_dict['OccurringServiceInstanceId'], 4)\n",
    "            self.embedding_RaisingEnvironment = nn.Embedding(max_len_dict['RaisingEnvironment'], 4)\n",
    "            self.embedding_RaisingDatacenter = nn.Embedding(max_len_dict['RaisingDatacenter'], 4)\n",
    "            self.embedding_RaisingDeviceName = nn.Embedding(max_len_dict['RaisingDeviceName'], 4)\n",
    "            self.embedding_RaisingDeviceGroup = nn.Embedding(max_len_dict['RaisingDeviceGroup'], 4)\n",
    "            self.embedding_IncidentType = nn.Embedding(max_len_dict['IncidentType'], 4)\n",
    "            self.embedding_SourceType = nn.Embedding(max_len_dict['SourceType'], 4)\n",
    "            self.embedding_SourceOrigin = nn.Embedding(max_len_dict['SourceOrigin'], 4)\n",
    "            self.embedding_variable = nn.Embedding(max_len_dict['variable'], 64)\n",
    "            \n",
    "            for i in range(len(ngram_kernel_sizes)):\n",
    "                conv = nn.Conv1D(nums_channels[i],\n",
    "                    kernel_size=ngram_kernel_sizes[i],\n",
    "                    strides=1,\n",
    "                    activation='relu')\n",
    "                pool = nn.GlobalMaxPool1D()\n",
    "                setattr(self, f'conv_{i}', conv) \n",
    "                setattr(self, f'pool_{i}', pool)\n",
    "\n",
    "            self.temp_pool = nn.GlobalMaxPool1D()\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.attention = self.attention_model(sequence_len)\n",
    "            self.dropout2 = nn.Dropout(dropout_rate)\n",
    "            self.dense = nn.Dense(32)  \n",
    "            self.decoder = nn.Dense(num_outputs)\n",
    "       \n",
    "        def forward(self, inputs, context, MonitorId, OccurringEnvironment, OccurringDeviceName, IncidentType, SourceType, SourceOrigin, variable):\n",
    "\n",
    "            embeddings = self.embedding(inputs)\n",
    "            embedding_context = self.embedding(context)\n",
    "            embedd_MonitorId = self.embedding_MonitorId(MonitorId)\n",
    "            embedd_OccurringEnvironment = self.embedding_OccurringEnvironment(OccurringEnvironment)\n",
    "            embedd_TenantSilos = self.embedding_TenantSilos(TenantSilos)\n",
    "            embedd_OccurringDatacenter = self.embedding_OccurringDatacenter(OccurringDatacenter)\n",
    "            embedd_OccurringDeviceName = self.embedding_OccurringDeviceName(OccurringDeviceName)\n",
    "            embedd_OccurringDeviceGroup = self.embedding_OccurringDeviceGroup(OccurringDeviceGroup)\n",
    "            embedd_OccurringServiceInstanceId = self.embedding_OccurringServiceInstanceId(OccurringServiceInstanceId)\n",
    "            embedd_RaisingEnvironment = self.embedding_RaisingEnvironment(RaisingEnvironment)\n",
    "            embedd_RaisingDatacenter = self.embedding_RaisingDatacenter(RaisingDatacenter)\n",
    "            embedd_RaisingDeviceName = self.embedding_RaisingDeviceName(RaisingDeviceName)\n",
    "            embedd_RaisingDeviceGroup = self.embedding_RaisingDeviceGroup(RaisingDeviceGroup)\n",
    "            embedd_IncidentType = self.embedding_IncidentType(IncidentType)\n",
    "            embedd_SourceType = self.embedding_SourceType(SourceType)\n",
    "            embedd_SourceOrigin = self.embedding_SourceOrigin(SourceOrigin)\n",
    "            embedd_variable = self.embedding_variable(variable)\n",
    "            \n",
    "            # TARGET\n",
    "            embeddings = embeddings.transpose((0,2,1)) \n",
    "            encoding = [\n",
    "                nd.flatten(self.get_pool(i)(self.get_conv(i)(embeddings)))\n",
    "                for i in range(len(self.ngram_kernel_sizes))]\n",
    "            encoding = nd.concat(*encoding, dim=1)\n",
    "            \n",
    "            # CONTEXT\n",
    "            context_conved = []\n",
    "            for line in embedding_context.transpose((1,0,3,2)):\n",
    "                c_encoding = [\n",
    "                            nd.flatten(self.get_pool(i)(self.get_conv(i)(line)))\n",
    "                            for i in range(len(self.ngram_kernel_sizes))]\n",
    "                c_encoding = nd.concat(*c_encoding, dim=1)\n",
    "                context_conved.append(nd.expand_dims(c_encoding,axis=1))\n",
    "            \n",
    "            context_conved.append(nd.expand_dims(encoding,axis=1))\n",
    "            context_conved = nd.concat(*context_conved, dim=1).transpose((1,0,2))            \n",
    "            query_encoding = nd.broadcast_axis(encoding.expand_dims(0), axis=0, size=context_conved.shape[0])\n",
    "            \n",
    "            enc_and_dec_states = nd.concat(context_conved, query_encoding, dim=2)\n",
    "            e = self.attention(enc_and_dec_states)\n",
    "            alpha = nd.softmax(e, axis=0) \n",
    "            context_vector = (alpha * context_conved).sum(axis=0) \n",
    "            \n",
    "            outputs = self.dense(self.dropout(\n",
    "                nd.concat(\n",
    "                encoding,\n",
    "                context_vector,\n",
    "                embedd_MonitorId,\n",
    "                embedd_OccurringEnvironment,\n",
    "                embedd_TenantSilos,\n",
    "                embedd_OccurringDatacenter,\n",
    "                embedd_OccurringDeviceName,\n",
    "                embedd_OccurringDeviceGroup,\n",
    "                embedd_OccurringServiceInstanceId,\n",
    "                embedd_RaisingEnvironment,\n",
    "                embedd_RaisingDatacenter,\n",
    "                embedd_RaisingDeviceName,\n",
    "                embedd_RaisingDeviceGroup,\n",
    "                embedd_IncidentType,\n",
    "                embedd_SourceType,\n",
    "                embedd_SourceOrigin,\n",
    "                embedd_variable,\n",
    "                dim=1)               \n",
    "            ))\n",
    "            \n",
    "            outs = self.decoder(self.dropout2(outputs))\n",
    "            return outs\n",
    "        \n",
    "        def get_conv(self, i):\n",
    "            return getattr(self, f'conv_{i}')\n",
    "        \n",
    "        def get_pool(self, i):\n",
    "            return getattr(self, f'pool_{i}')\n",
    "        \n",
    "        def get_conv_c(self, i):\n",
    "            return getattr(self, f'conv_c_{i}')\n",
    "        \n",
    "        def get_pool_c(self, i):\n",
    "            return getattr(self, f'pool_c_{i}')\n",
    "        \n",
    "        def attention_model(self, attention_size):\n",
    "            model = nn.Sequential()\n",
    "            model.add(nn.Dense(attention_size, activation='tanh', use_bias=False, flatten=False),\n",
    "                      nn.Dense(1, use_bias=False, flatten=False))\n",
    "            return model\n",
    "    \n",
    "    def eval_model(train_iter):\n",
    "        l_sum = 0\n",
    "        l_n = 0\n",
    "        pred_probs = []\n",
    "        labels_result = []\n",
    "        for data, context, label, weights, MonitorId, OccurringEnvironment, OccurringDeviceName, IncidentType, SourceType, SourceOrigin, variable in train_iter:\n",
    "            X = data.as_in_context(ctx)\n",
    "            C = context.as_in_context(ctx)\n",
    "            y = label.as_in_context(ctx).T\n",
    "            \n",
    "            MonitorId = MonitorId.as_in_context(ctx)\n",
    "            OccurringEnvironment = OccurringEnvironment.as_in_context(ctx)\n",
    "            TenantSilos = TenantSilos.as_in_context(ctx)\n",
    "            OccurringDatacenter = OccurringDatacenter.as_in_context(ctx)\n",
    "            OccurringDeviceName = OccurringDeviceName.as_in_context(ctx)\n",
    "            OccurringDeviceGroup = OccurringDeviceGroup.as_in_context(ctx)\n",
    "            OccurringServiceInstanceId = OccurringServiceInstanceId.as_in_context(ctx)\n",
    "            RaisingEnvironment = RaisingEnvironment.as_in_context(ctx)\n",
    "            RaisingDatacenter = RaisingDatacenter.as_in_context(ctx)\n",
    "            RaisingDeviceName = RaisingDeviceName.as_in_context(ctx)\n",
    "            RaisingDeviceGroup = RaisingDeviceGroup.as_in_context(ctx)\n",
    "            IncidentType = IncidentType.as_in_context(ctx)\n",
    "            SourceType = SourceType.as_in_context(ctx)\n",
    "            SourceOrigin = SourceOrigin.as_in_context(ctx)\n",
    "            variable = variable.as_in_context(ctx)                      \n",
    "            \n",
    "            y_pred = net(X, C, MonitorId, OccurringEnvironment, OccurringDeviceName, IncidentType, SourceType, SourceOrigin, variable)\n",
    "            l = loss(y_pred, y)\n",
    "            labels_result.extend(list(y.astype('int').asnumpy()))\n",
    "            \n",
    "            for line in y_pred:         \n",
    "                pred_probs.extend(list(nd.softmax(line)[1].astype('float').asnumpy()))\n",
    "                \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(labels_result, pred_probs)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        return 1, roc_auc\n",
    "    \n",
    "    def eval_model_test(train_iter):\n",
    "        l_sum = 0\n",
    "        l_n = 0\n",
    "        pred_result = []\n",
    "        labels_result = []\n",
    "        pred_probs = []\n",
    "        \n",
    "        for data, context, label, weights, MonitorId, OccurringEnvironment, OccurringDeviceName, IncidentType, SourceType, SourceOrigin, variable in train_iter:\n",
    "            X = data.as_in_context(ctx)\n",
    "            C = context.as_in_context(ctx)\n",
    "            y = label.as_in_context(ctx).T\n",
    "            \n",
    "            MonitorId = MonitorId.as_in_context(ctx)\n",
    "            OccurringEnvironment = OccurringEnvironment.as_in_context(ctx)\n",
    "            TenantSilos = TenantSilos.as_in_context(ctx)\n",
    "            OccurringDatacenter = OccurringDatacenter.as_in_context(ctx)\n",
    "            OccurringDeviceName = OccurringDeviceName.as_in_context(ctx)\n",
    "            OccurringDeviceGroup = OccurringDeviceGroup.as_in_context(ctx)\n",
    "            OccurringServiceInstanceId = OccurringServiceInstanceId.as_in_context(ctx)\n",
    "            RaisingEnvironment = RaisingEnvironment.as_in_context(ctx)\n",
    "            RaisingDatacenter = RaisingDatacenter.as_in_context(ctx)\n",
    "            RaisingDeviceName = RaisingDeviceName.as_in_context(ctx)\n",
    "            RaisingDeviceGroup = RaisingDeviceGroup.as_in_context(ctx)\n",
    "            IncidentType = IncidentType.as_in_context(ctx)\n",
    "            SourceType = SourceType.as_in_context(ctx)\n",
    "            SourceOrigin = SourceOrigin.as_in_context(ctx)\n",
    "            variable = variable.as_in_context(ctx)                      \n",
    "            \n",
    "            y_pred= net(X, C, MonitorId, OccurringEnvironment, OccurringDeviceName, IncidentType, SourceType, SourceOrigin, variable)           \n",
    "            l = loss(y_pred, y)\n",
    "            pred_result.extend(list(nd.argmax(nd.softmax(y_pred), axis=1).astype('int').asnumpy()))\n",
    "            labels_result.extend(list(y.astype('int').asnumpy()))\n",
    "            for line in y_pred:         \n",
    "                pred_probs.extend(list(nd.softmax(line)[1].astype('float').asnumpy()))\n",
    "        \n",
    "        return metrics.precision_score(labels_result, pred_result, average='macro'), \\\n",
    "                    metrics.recall_score(labels_result, pred_result, average='macro'),\\\n",
    "                    metrics.f1_score(labels_result, pred_result, average='macro'),\\\n",
    "                    metrics.accuracy_score(labels_result, pred_result), \\\n",
    "                    pred_result,pred_probs,labels_result\n",
    "    \n",
    "    ROOTPATH = \"PATH\"\n",
    "    samples = pickle.load(open(ROOTPATH + project_name + '_train_title_summary.pkl','rb'))\n",
    "    samples_test = pickle.load(open(ROOTPATH + project_name + '_test_title_summary.pkl','rb'))\n",
    "    \n",
    "    train = {}\n",
    "    test = {}\n",
    "    \n",
    "    samples['Label_Multi'] = samples['Label'].apply(lambda x: getMultiLabel(x))\n",
    "    samples_test['Label_Multi'] = samples_test['Label'].apply(lambda x: getMultiLabel(x))\n",
    "    \n",
    "    samples['Label_Binary'] = samples['Label'].apply(lambda x: getBinaryLabel(x))\n",
    "    samples_test['Label_Binary'] = samples_test['Label'].apply(lambda x: getBinaryLabel(x))\n",
    "    \n",
    "    samples['variable'] = samples['variable'].apply(lambda x: clean_str(str(x)))\n",
    "    samples_test['variable'] = samples_test['variable'].apply(lambda x: clean_str(str(x)))\n",
    "    \n",
    "    train['sentences'] = list(samples['tokenized'])\n",
    "    train['labels'] = list(samples['Label_Binary'])\n",
    "    \n",
    "    # multi weight for train\n",
    "    weight_dict = {}\n",
    "    total = len(samples['Label_Multi'])\n",
    "    min_sample = samples['Label_Multi'].value_counts().min()\n",
    "    for index,x in samples['Label_Multi'].value_counts().iteritems():\n",
    "        weight_dict[index] = 1 - float(x) / total\n",
    "    \n",
    "    weightMatrix = nd.array([weight_dict[x] for x in weight_dict])\n",
    "    weightMatrix = nd.softmax(weightMatrix)\n",
    "    \n",
    "    i=0\n",
    "    for x in weight_dict:\n",
    "        weight_dict[x] = weightMatrix[i].astype('float').asnumpy()[0]\n",
    "        i += 1\n",
    "    \n",
    "    train['weights'] = []\n",
    "    for line in samples['Label_Multi']:\n",
    "        train['weights'].append(weight_dict[line])\n",
    "    train['weights'] =  nd.array(train['weights'])\n",
    "                        \n",
    "    test['sentences'] = list(samples_test['tokenized']) \n",
    "    test['labels'] = list(samples_test['Label_Binary'])\n",
    "    \n",
    "    test['weights'] = []\n",
    "    for line in samples_test['Label_Multi']:  \n",
    "         test['weights'].append(weight_dict[line])\n",
    "    test['weights'] = nd.array(test['weights'])\n",
    "    \n",
    "    max_len = 80\n",
    "    while (np.sum(np.array(list(map(len,train['sentences'])))<max_len)/len(train['sentences'])<0.95):\n",
    "        max_len += 10   \n",
    "    \n",
    "    for c in feature_columns:\n",
    "        tmp_dict = {}\n",
    "        idx = 0\n",
    "        for x in list(samples[c].drop_duplicates()):\n",
    "            tmp_dict[x] = idx\n",
    "            idx += 1\n",
    "            \n",
    "        for x in list(samples_test[c].drop_duplicates()):\n",
    "            if x not in tmp_dict:\n",
    "                tmp_dict[x] = idx\n",
    "                idx += 1\n",
    "            \n",
    "        train[c] = nd.array(list(samples[c].replace(tmp_dict)))\n",
    "        test[c] = nd.array(list(samples_test[c].replace(tmp_dict)))\n",
    "\n",
    "    token_counter = collections.Counter()\n",
    "    token_counter = count_token(train['sentences'], token_counter)\n",
    "    vocab = text.vocab.Vocabulary(token_counter, unknown_token='<unk>', min_freq=2, reserved_tokens=None, most_freq_count=30000)\n",
    "    \n",
    "    ctx = mx.gpu(1)\n",
    "    from gensim.models import FastText\n",
    "    model = FastText(train['sentences'] + test['sentences'], min_count=1, size=200, iter=10, workers=8)\n",
    "    w2v = []\n",
    "    for line in vocab.token_to_idx:\n",
    "        if line in model.wv:\n",
    "            w2v.append(model.wv[line])\n",
    "        else:\n",
    "            w2v.append(np.zeros(200))\n",
    "    w2v = nd.array(w2v)\n",
    "    print('w2v success', w2v.shape)\n",
    "    \n",
    "    train['features'] = encode_samples(train['sentences'], vocab)\n",
    "    train['features'] = nd.array(pad_samples(train['features'], max_len, 0))\n",
    "                                \n",
    "    test['features'] = encode_samples(test['sentences'], vocab)\n",
    "    test['features'] = nd.array(pad_samples(test['features'], max_len, 0))\n",
    "    \n",
    "    max_len_dict = {}\n",
    "    for c in feature_columns:\n",
    "        max_len = int(np.max(train[c]).asnumpy()) \n",
    "        if max_len <int(np.max(test[c]).asnumpy()) :\n",
    "            max_len = int(np.max(test[c]).asnumpy())\n",
    "        max_len_dict[c] = max_len\n",
    "    print('encode_samples success')\n",
    "    \n",
    "    sequence_length = 10\n",
    "    \n",
    "    train['samples'] = []\n",
    "    for index,line in enumerate(train['features']):\n",
    "        if index<=sequence_length:\n",
    "            continue\n",
    "        \n",
    "        if index % 100000==0:\n",
    "            print(index)\n",
    "        \n",
    "        cur_samples = []\n",
    "        for i in range(-sequence_length,-1,1):\n",
    "            \n",
    "            if index + i >= 0 and index + i<len(train['features']):\n",
    "                cur_samples.append(nd.expand_dims(train['features'][index + i],axis=0))\n",
    "        \n",
    "        train['samples'].append(nd.concat(*cur_samples,dim=0))\n",
    "    \n",
    "    test['samples'] = []\n",
    "    for index,line in enumerate(test['features']):\n",
    "        if index<=sequence_length:\n",
    "            continue\n",
    "        \n",
    "        if index % 100000==0:\n",
    "            print(index)\n",
    "        \n",
    "        cur_samples = []\n",
    "        for i in range(-sequence_length,-1,1):\n",
    "            if index + i >= 0 and index + i<len(test['features']):\n",
    "                cur_samples.append(nd.expand_dims(test['features'][index + i],axis=0))\n",
    "        test['samples'].append(nd.concat(*cur_samples,dim=0))\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 10\n",
    "    sentence_hidden = 32\n",
    "    batch_size = 512\n",
    "    embed_size = 200\n",
    "    ngram_kernel_sizes = [3,4,5]\n",
    "    nums_channels = [100,100,100]\n",
    "    \n",
    "    net = TextCNN(vocab, embed_size, ngram_kernel_sizes, nums_channels, 2, max_len_dict, sequence_length)\n",
    "    net.initialize(init.Xavier(), ctx=ctx)\n",
    "    net.embedding.weight.set_data(w2v.as_in_context(ctx))\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        train['features'][sequence_length + 1:],\n",
    "        train['samples'], \n",
    "        train['labels'][sequence_length + 1:],\n",
    "        train['weights'][sequence_length + 1:], \n",
    "        train['MonitorId'][sequence_length + 1:],\n",
    "        train['OccurringEnvironment'][sequence_length + 1:],\n",
    "        train['TenantSilos'][sequence_length + 1:],\n",
    "        train['OccurringDatacenter'][sequence_length + 1:],\n",
    "        train['OccurringDeviceName'][sequence_length + 1:],\n",
    "        train['OccurringDeviceGroup'][sequence_length + 1:],\n",
    "        train['OccurringServiceInstanceId'][sequence_length + 1:],\n",
    "        train['RaisingEnvironment'][sequence_length + 1:],\n",
    "        train['RaisingDatacenter'][sequence_length + 1:],\n",
    "        train['RaisingDeviceName'][sequence_length + 1:],\n",
    "        train['RaisingDeviceGroup'][sequence_length + 1:],\n",
    "        train['IncidentType'][sequence_length + 1:],\n",
    "        train['SourceType'][sequence_length + 1:],\n",
    "        train['SourceOrigin'][sequence_length + 1:],\n",
    "        train['variable'][sequence_length + 1:]\n",
    "    ), batch_size, shuffle=True)\n",
    "    \n",
    "    test_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        test['features'][sequence_length + 1:],\n",
    "        test['samples'], \n",
    "        test['labels'][sequence_length + 1:],\n",
    "        test['weights'][sequence_length + 1:], \n",
    "        test['MonitorId'][sequence_length + 1:],\n",
    "        test['OccurringEnvironment'][sequence_length + 1:],\n",
    "        test['TenantSilos'][sequence_length + 1:],\n",
    "        test['OccurringDatacenter'][sequence_length + 1:],\n",
    "        test['OccurringDeviceName'][sequence_length + 1:],\n",
    "        test['OccurringDeviceGroup'][sequence_length + 1:],\n",
    "        test['OccurringServiceInstanceId'][sequence_length + 1:],\n",
    "        test['RaisingEnvironment'][sequence_length + 1:],\n",
    "        test['RaisingDatacenter'][sequence_length + 1:],\n",
    "        test['RaisingDeviceName'][sequence_length + 1:],\n",
    "        test['RaisingDeviceGroup'][sequence_length + 1:],\n",
    "        test['IncidentType'][sequence_length + 1:],\n",
    "        test['SourceType'][sequence_length + 1:],\n",
    "        test['SourceOrigin'][sequence_length + 1:],\n",
    "        test['variable'][sequence_length + 1:]\n",
    "    ), batch_size, shuffle=True)\n",
    "    \n",
    "    best_auc = 0\n",
    "    start = time.clock()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        l_sum = 0\n",
    "        l_n = 0\n",
    "        for data, context, label, weight, MonitorId, OccurringEnvironment, OccurringDeviceName, IncidentType, SourceType, SourceOrigin, variable in train_iter:\n",
    "            X = data.as_in_context(ctx)\n",
    "            C = context.as_in_context(ctx)\n",
    "            y = label.as_in_context(ctx).T\n",
    "            w = weight.as_in_context(ctx).T\n",
    "            \n",
    "            MonitorId = MonitorId.as_in_context(ctx)\n",
    "            OccurringEnvironment = OccurringEnvironment.as_in_context(ctx)\n",
    "            TenantSilos = TenantSilos.as_in_context(ctx)\n",
    "            OccurringDatacenter = OccurringDatacenter.as_in_context(ctx)\n",
    "            OccurringDeviceName = OccurringDeviceName.as_in_context(ctx)\n",
    "            OccurringDeviceGroup = OccurringDeviceGroup.as_in_context(ctx)\n",
    "            OccurringServiceInstanceId = OccurringServiceInstanceId.as_in_context(ctx)\n",
    "            RaisingEnvironment = RaisingEnvironment.as_in_context(ctx)\n",
    "            RaisingDatacenter = RaisingDatacenter.as_in_context(ctx)\n",
    "            RaisingDeviceName = RaisingDeviceName.as_in_context(ctx)\n",
    "            RaisingDeviceGroup = RaisingDeviceGroup.as_in_context(ctx)\n",
    "            IncidentType = IncidentType.as_in_context(ctx)\n",
    "            SourceType = SourceType.as_in_context(ctx)\n",
    "            SourceOrigin = SourceOrigin.as_in_context(ctx)\n",
    "            variable = variable.as_in_context(ctx)\n",
    "            \n",
    "            with autograd.record():\n",
    "                y_hat = net(X, C, MonitorId, OccurringEnvironment, OccurringDeviceName, IncidentType, SourceType, SourceOrigin, variable)\n",
    "                l = loss(y_hat, y, w)\n",
    "            \n",
    "            l.backward()\n",
    "            trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        \n",
    "        train_loss, train_auc = eval_model(train_iter)\n",
    "        val_loss, val_auc = eval_model(test_iter)\n",
    "        if val_auc>=best_auc:\n",
    "            net.save_parameters('DeepTIP.model')\n",
    "            best_auc = val_auc\n",
    "        print('epoch %d, train loss %.4f, auc %.2f; val loss %.6f, auc %.4f' \n",
    "              % (epoch, train_loss, train_auc, val_loss, val_auc))\n",
    "    \n",
    "    end = time.clock()\n",
    "    print('train success')\n",
    "   \n",
    "    net.load_parameters('DeepTIP.model')\n",
    "    start_p = time.clock()\n",
    "    pre, recall, f1, acc1,pred_result,pred_probs,labels_result = eval_model_test(test_iter)\n",
    "    end_p = time.clock()\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels_result, pred_probs)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)  ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('DeepTip-Binary')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    csv_write.writerow([project_name,'DeepTip', roc_auc, pre, recall, f1, acc1,end-start,end_p-start_p])\n",
    "    pickle.dump([pred_result, labels_result],open('result/' + project_name + '_dl_em.proba.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
